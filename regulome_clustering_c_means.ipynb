{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leiden followed by C means clustering of regulome\n",
    "\n",
    "First make a proof of concept using igraph\n",
    "\n",
    "Questions:\n",
    "- How to measure distance\n",
    "- How to measure membership\n",
    "- How to treat small clusters - leave and expand or remove and add nodes to new?\n",
    "\n",
    "If it works:\n",
    "- make faster implementation\n",
    "\n",
    "\n",
    "Problems:\n",
    "- current distance measure: \n",
    "    - favors large clusters and nodes with high degree\n",
    "    - maybe make measure of distance based on the number of nodes in the cluster\n",
    "        - in a cluster of 10, 10 edges would make highly conncted to cluster, but in cluster of 100, 10 edges might not.\n",
    "        - ideally, if node connected to all nodes, with 1 PPV - distance should be nearing 0, if no connection should be 1\n",
    "        - 1 - (n_nodes_cluster/sum(distances))\n",
    "            - this will favor smaller clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import igraph as ig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from compress_pickle import load, dump\n",
    "from statistics import mean\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def open_pickle(file):\n",
    "    with open(path+file, 'rb') as pickle_file:\n",
    "        return load(path=pickle_file, compression='infer')\n",
    "\n",
    "path = './files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load regulome\n",
    "regulome_network_edges = pd.read_csv(filepath_or_buffer=path+'/human_regulome_pd.gz', compression='infer')\n",
    "regulome_network_edges = regulome_network_edges.astype({'ProteinAid': 'str', 'ProteinBid':'str'})\n",
    "\n",
    "regulome_graph = ig.Graph.DataFrame(regulome_network_edges, directed=False, use_vids=False)\n",
    "# msigdb_c3_tft_dict = open_pickle('msigdb_c3_tft_dict.pkl')\n",
    "proteins = pd.concat([regulome_network_edges['ProteinAid'], regulome_network_edges['ProteinBid']]).unique()\n",
    "\n",
    "# del regulome_network_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leiden_clustering(graph, res, b, n_iter):\n",
    "    return graph.community_leiden(objective_function='modularity',\n",
    "                            weights='PPV',\n",
    "                            resolution=res,\n",
    "                            beta=b,\n",
    "                            n_iterations=n_iter)\n",
    "\n",
    "leiden_clusters = leiden_clustering(regulome_graph, 4, 0.1, 10)\n",
    "clusters_dict = {n:set(regulome_graph.vs[node]['name'] for node in cluster) for (n, cluster) in enumerate(leiden_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.histplot(leiden_clusters.sizes())\n",
    "fig.set_title(\"Cluster size distribution of Leiden clusters\")\n",
    "fig.set_xlabel(\"Cluster Size\")\n",
    "fig.savefig(\"./images/LiedenClustersDistr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'path_lengths_dict.gz', 'rb') as file:\n",
    "    path_lengths_dict = load(file, compression='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "### Functions for calculating distance from node to cluster\n",
    "def inverse_sum_PPV_dist(neighbors, cluster):\n",
    "    connected_nodes = cluster.intersection(set(neighbors.keys()))\n",
    "    if len(connected_nodes) == 0:\n",
    "        return 1.5 # determine the standard distance - 1.5/min(weight) \n",
    "    \n",
    "    weights_sum = sum([neighbors[node] for node in connected_nodes])\n",
    "    if weights_sum == 0:\n",
    "        return 1.5\n",
    "    \n",
    "    return 1 / weights_sum\n",
    "\n",
    "\n",
    "def path_length_dist(path_lengths, cluster):    \n",
    "    return mean([path_lengths[node] for node in cluster])\n",
    "\n",
    "\n",
    "def node_c_distances(neighbors, clusters, distance_measure='dist_sum'): # add weight formula argument:\n",
    "    node_distances = dict()\n",
    "\n",
    "    for (cluster_id, cluster) in clusters.items():\n",
    "        match distance_measure:\n",
    "            case 'dist_sum':\n",
    "                node_distances[cluster_id] =  inverse_sum_PPV_dist(neighbors, cluster)# set 1.5 as max distance, should I set other to more ?\n",
    "            case 'path_len':\n",
    "                node_distances[cluster_id] = path_length_dist(neighbors, cluster)\n",
    "            case _:\n",
    "                return # quit function if wrong function name\n",
    "    return node_distances\n",
    "\n",
    "\n",
    "### Function to calculate membership values for node\n",
    "def calc_membership(node_distances, m):\n",
    "    node_memberships = dict()\n",
    "\n",
    "    for (cluster_id, d) in node_distances.items():         \n",
    "        fuzzy_exponent = (1 / (m - 1))\n",
    "\n",
    "        # give better name - denomintation or membership function  \n",
    "        membership_den = [(d / other_d) ** fuzzy_exponent for other_d in node_distances.values()]\n",
    "        node_memberships[cluster_id] = 1 / sum(membership_den)\n",
    "\n",
    "    return node_memberships\n",
    "\n",
    "\n",
    "### Function to calcualte node addition to objective function J (optimization function)\n",
    "def optimization_function(node_distances, node_memberships, m): # change function name\n",
    "    node_J = 0\n",
    "    for cluster_id in node_distances.keys():\n",
    "        node_J += node_memberships[cluster_id]**m * node_distances[cluster_id]**2\n",
    "    return node_J\n",
    "    \n",
    "\n",
    "### Function for iteration of each node, calculates distance, membership and J\n",
    "# Necessary for parallelization\n",
    "def node_iteration(node, neighbors, clusters, m, optimize):\n",
    "    node_distances = node_c_distances(neighbors, clusters) # returns dictionary with distance to each cluster\n",
    "    node_memberships = calc_membership(node_distances, m) # adds memberships of node to dictionary\n",
    "    if optimize:\n",
    "        Ji = optimization_function(node_distances, node_memberships, m)\n",
    "        return node, node_distances, node_memberships, Ji\n",
    "    return node, node_distances, node_memberships\n",
    "\n",
    "\n",
    "def gather_neighbors(graph, nodes, distance_measure='dist_sum'):\n",
    "    match distance_measure:\n",
    "        case 'dist_sum':\n",
    "            return {\n",
    "                node : {\n",
    "                    graph.vs[neighbor]['name'] : graph.es[graph.get_eid(node, neighbor)]['PPV'] for neighbor in graph.neighbors(node)\n",
    "                    } \n",
    "                    for node in nodes\n",
    "                }\n",
    "        case _:\n",
    "            return \n",
    "\n",
    "\n",
    "def update_clusters_perc(memberships_dict, clusters, percentile=95):\n",
    "    print(\"Updating Clusters\")\n",
    "    for (node, memberships) in memberships_dict.items():\n",
    "        perc_lim = np.percentile(list(memberships.values()), percentile)\n",
    "        node_clusters = [cluster_id for (cluster_id, membership) in memberships.items() if membership >= perc_lim]\n",
    "        for cluster_id in node_clusters:\n",
    "            clusters[cluster_id].add(node)\n",
    "    return clusters\n",
    "\n",
    "### Main function\n",
    "# path_lengths only necessarz when using the path_len distance measure\n",
    "def network_c_means(graph, clusters, m, optimize=False, distance_measure='dist_sum', path_lengths=None):\n",
    "    # add leiden here after \n",
    "    nodes = [node['name'] for node in graph.vs]\n",
    "    memberships_dict = dict()\n",
    "    \n",
    "    if distance_measure == 'path_len':\n",
    "        if path_lengths == None:\n",
    "            print(\"Must give dictioonary containing path lengths using path_lengths argument\")\n",
    "            return\n",
    "        connections_dict = path_lengths # maybe change name, cuase connections_dict kinda silly\n",
    "        del path_lengths\n",
    "    else:\n",
    "        connections_dict = gather_neighbors(graph, nodes, distance_measure)\n",
    "    if optimize:\n",
    "        Js = list()\n",
    "\n",
    "    del graph\n",
    "\n",
    "    for i in range(1,6):\n",
    "        print(f'Iteration {i} of 5')\n",
    "\n",
    "        results = Parallel(n_jobs=6)(delayed(node_iteration)(node, connections_dict[node], clusters, m, optimize) for node in tqdm(nodes, desc=\"Calculating\"))\n",
    "        \n",
    "        Js.append(sum(result[3] for result in results)) # maybe make results into a named tuple\n",
    "        memberships_dict = {result[0] : result[2] for result in results}\n",
    "        \n",
    "        clusters = update_clusters_perc(memberships_dict, clusters, percentile=95)\n",
    "\n",
    "        del results, memberships_dict\n",
    "        clear_output()\n",
    "\n",
    "    return clusters, Js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/jupyter-nbextension\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 270, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 988, in start\n",
      "    super(NBExtensionApp, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 259, in start\n",
      "    self.subapp.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 896, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 872, in toggle_nbextension_python\n",
      "    logger=self.log)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 483, in enable_nbextension_python\n",
      "    logger=logger)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 380, in _set_nbextension_state_python\n",
      "    m, nbexts = _get_nbextension_metadata(module)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 1122, in _get_nbextension_metadata\n",
      "    m = import_item(module)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/utils/importstring.py\", line 42, in import_item\n",
      "    return __import__(parts[0])\n",
      "ImportError: No module named widgetsnbextension\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 5\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork_c_means\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregulome_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_measure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdist_sum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 116\u001b[0m, in \u001b[0;36mnetwork_c_means\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of 5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)(delayed(node_iteration)(node, connections_dict[node], clusters, m, optimize) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCalculating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    118\u001b[0m     Js\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(result[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results)) \u001b[38;5;66;03m# maybe make results into a named tuple\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     memberships_dict \u001b[38;5;241m=\u001b[39m {result[\u001b[38;5;241m0\u001b[39m] : result[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results}\n",
      "File \u001b[0;32m~/miniconda3/envs/regulome/lib/python3.13/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/regulome/lib/python3.13/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "results = network_c_means(regulome_graph, clusters_dict, 2, optimize=True, distance_measure='dist_sum', path_lengths=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "nodes = [node for node in regulome_graph.vs['name']]\n",
    "\n",
    "with open(path+'path_lengths.gz', 'rb') as file:\n",
    "    path_lengths = load(file, compression='infer')\n",
    "\n",
    "path_lengths = path_lengths[0]\n",
    "path_lengths_dict = dict()\n",
    "n_nodes = len(nodes)\n",
    "for (node_number, node) in enumerate(nodes):\n",
    "    clear_output()\n",
    "    print(f'{node_number}/{n_nodes}')\n",
    "    path_lengths_dict[node] = {\n",
    "        other_node: round(path_lengths, 2)\n",
    "        for (other_node, path_lengths) in zip(nodes, path_lengths[:n_nodes])\n",
    "    }\n",
    "    path_lengths = path_lengths[n_nodes:]\n",
    "del path_lengths\n",
    "\n",
    "with open(path+'path_lengths_dict.gz', 'wb') as file:\n",
    "    dump(path_lengths_dict,path+'path_lengths_dict.gz', compression='infer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = network_c_means_connections(regulome_graph, ['348999'], clusters_dict, 2, distance_measure='dist_sum', path_lengths=path_lengths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "protein_edges = pd.concat([regulome_network_edges['ProteinAid'], regulome_network_edges['ProteinBid']]).value_counts()\n",
    "most_connected_proteins = tuple(protein_edges.index[:5])\n",
    "least_connected_proteins = tuple(protein_edges.index[-5:])\n",
    "\n",
    "def network_c_means_connections(graph, nodes, clusters, m, optimize=False, distance_measure='dist_sum', path_lengths=None):\n",
    "    # add leiden here after \n",
    "        \n",
    "    if distance_measure == 'path_len':\n",
    "        if path_lengths == None:\n",
    "            print(\"Must give dictioonary containing path lengths using path_lengths argument\")\n",
    "            return\n",
    "        connections_dict = path_lengths # maybe change name, cuase connections_dict kinda silly\n",
    "        del path_lengths\n",
    "    else:\n",
    "        connections_dict = gather_neighbors(graph, nodes, distance_measure)\n",
    "    if optimize:\n",
    "        J = 0\n",
    "\n",
    "    results = Parallel(n_jobs=4)(delayed(node_iteration)(node, connections_dict[node], clusters, m, optimize) for node in nodes)\n",
    "\n",
    "    memberships_dict = {result[0] : {\n",
    "            cluster_id : log10(r) for (cluster_id, r) in result[2].items()\n",
    "            }\n",
    "        for result in results\n",
    "        }\n",
    "    return memberships_dict\n",
    "\n",
    "    # distances_dict = {result[0]: tuple(r for r in result[1].values()) for result in results}\n",
    "    # return distances_dict\n",
    "\n",
    "m_values = [1.1, 2, 3, 4, 5, 6]\n",
    "\n",
    "most_connected_memberships = dict()\n",
    "for m in m_values:\n",
    "    most_connected_memberships[m] = network_c_means_connections(regulome_graph, most_connected_proteins, clusters_dict, m, distance_measure='dist_sum', path_lengths=path_lengths_dict)\n",
    "\n",
    "\n",
    "least_connected_memberships = dict()\n",
    "for m in m_values:\n",
    "    least_connected_memberships[m] = network_c_means_connections(regulome_graph, least_connected_proteins, clusters_dict, m, distance_measure='dist_sum', path_lengths=path_lengths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_connected_memberships\n",
    "\n",
    "# Manually extract the keys and values\n",
    "rows = [\n",
    "    (m, protein, membership)\n",
    "    for (m, proteins) in most_connected_memberships.items()\n",
    "    for (protein, memberships) in proteins.items()\n",
    "    for membership in memberships\n",
    "    if m not in [1.1] \n",
    "]\n",
    "\n",
    "most_connected_df = pd.DataFrame(rows, columns=['fuzzy_m', 'Proteinid', 'membership'])\n",
    "\n",
    "fig = sns.FacetGrid(most_connected_df, col='fuzzy_m', row='Proteinid')\n",
    "fig.map(sns.histplot, 'membership')\n",
    "fig.figure.subplots_adjust(top=0.925)\n",
    "fig.set_xlabels('Membership (log10)')\n",
    "fig.figure.suptitle(\"Membership distribution of the 5 proteins with the highest degree\")\n",
    "fig.refline(y=1, c='r')\n",
    "# fig.savefig(\"./images/most_well_connected_sum_dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least_connected_memberships\n",
    "\n",
    "# Manually extract the keys and values\n",
    "rows = [\n",
    "    (m, protein, membership)\n",
    "    for (m, proteins) in least_connected_memberships.items()\n",
    "    for (protein, memberships) in proteins.items()\n",
    "    for membership in memberships\n",
    "]\n",
    "\n",
    "least_connected_df = pd.DataFrame(rows, columns=['fuzzy_m', 'Proteinid', 'membership'])\n",
    "\n",
    "fig = sns.FacetGrid(least_connected_df, col='fuzzy_m', row='Proteinid')\n",
    "fig.map(sns.histplot, 'membership')\n",
    "fig.figure.subplots_adjust(top=0.925)\n",
    "fig.set_xlabels('Membership (log10)')\n",
    "fig.figure.suptitle(\"Membership distribution of the 5 proteins with the lowest degree (more than 5)\")\n",
    "fig.refline(y=1, c='r')\n",
    "\n",
    "# fig.savefig(\"./images/least_connected_sum_dist_not1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(least_connected_memberships[1.1]['339837'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def calc_dist(graph, node, clusters):\n",
    "    node_distances = dict()\n",
    "    neighbors = set(graph.neighbors(node))\n",
    "\n",
    "    for (cluster_id, cluster) in clusters.items():\n",
    "        connected_nodes = set(neighbors).intersection(cluster)\n",
    "\n",
    "        if len(connected_nodes) == 0: # no need if no connections\n",
    "            node_distances[cluster_id] = 1\n",
    "            continue\n",
    "\n",
    "        # get weights of each connection to nodes in the cluster\n",
    "        weights = [graph.es(graph.get_eid(node, connected_node))['PPV'] for connected_node in connected_nodes]\n",
    "        weights_sum = sum(chain.from_iterable(weights))\n",
    "\n",
    "        if weights_sum == 0: # dont need this myself, but a thing to look at if want to ship out later\n",
    "            node_distances[cluster_id] = 1\n",
    "            continue\n",
    "\n",
    "        node_distances[cluster_id] = 1 / weights_sum\n",
    "    return node_distances\n",
    "\n",
    "\n",
    "def calc_membership(node_distances, m):\n",
    "    node_memberships = dict()\n",
    "\n",
    "    for (cluster_id, d) in node_distances.items():         \n",
    "        fuzzy_exponent = 2 / (m - 1)\n",
    "\n",
    "        # give better name - denomintation or membership function  \n",
    "        membership_den = [(d / other_d) ** fuzzy_exponent for other_d in node_distances.values()]\n",
    "\n",
    "        node_memberships[cluster_id] = 1 / sum(membership_den)\n",
    "\n",
    "    return node_memberships\n",
    "\n",
    "\n",
    "def optimization_function(node_distances, node_memberships, m): # change function name\n",
    "    node_J = 0\n",
    "    for cluster_id in node_distances.keys():\n",
    "        node_J += node_memberships[cluster_id]**m * node_distances[cluster_id]**2\n",
    "    return node_J\n",
    "    \n",
    "\n",
    "def network_c_means(graph, clusters, m, optimize=False):\n",
    "    # add leiden here after \n",
    "\n",
    "    membership_dict = dict()\n",
    "    distances_dict = dict()\n",
    "    \n",
    "    if optimize:\n",
    "        Js = list()\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        for node in graph.vs:\n",
    "            node = node['name']\n",
    "            node_distances = calc_dist(graph, node, clusters) # returns dictionary with distance to each cluster\n",
    "            node_memberships = calc_membership(node_distances, m) # adds memberships of node to dictionary\n",
    "\n",
    "            if optimize:\n",
    "                Js.append(optimization_function(node_distances, node_memberships, m))\n",
    "            distances_dict[node] = node_distances # maybe just add directly to dictionary\n",
    "            membership_dict[node] = node_memberships\n",
    "\n",
    "    \n",
    "\n",
    "    return Js\n",
    "\n",
    "memberships = network_c_means(regulome_graph, clusters_dict, 0.5, optimize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regulome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
